{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sliding Window Weighted Majority Voting Anomaly Globality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('../data/dfki-artificial-3000-unsupervised-ad.csv')\n",
    "\n",
    "# ==========================================\n",
    "# 1. ENSEMBLE GENERATOR\n",
    "# ==========================================\n",
    "def generate_ensemble_ranks_pyod(\n",
    "    df,\n",
    "    coordinate_cols=['attribute_1', 'attribute_2'],\n",
    "    k_range=range(5, 55, 1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs KNN and LOF over a range of k values to stabilize the anomaly scores.\n",
    "    Stores accumulated scores and returns averaged ranks.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    X = df[coordinate_cols].values\n",
    "\n",
    "    knn_accum = np.zeros(len(df))\n",
    "    lof_accum = np.zeros(len(df))\n",
    "    \n",
    "    for k in k_range:\n",
    "        # KNN: distance-based (global bias)\n",
    "        clf_knn = KNN(n_neighbors=k, method='mean') \n",
    "        clf_knn.fit(X)\n",
    "        knn_accum += clf_knn.decision_scores_\n",
    "        \n",
    "        # LOF: density-based (local bias)\n",
    "        clf_lof = LOF(n_neighbors=k)\n",
    "        clf_lof.fit(X)\n",
    "        lof_accum += clf_lof.decision_scores_\n",
    "\n",
    "    # Store accumulated scores\n",
    "    df['knn_score'] = knn_accum\n",
    "    df['lof_score'] = lof_accum\n",
    "\n",
    "    # Convert to ranks (1 = most anomalous)\n",
    "    df['knn_rank'] = pd.Series(knn_accum).rank(ascending=False)\n",
    "    df['lof_rank'] = pd.Series(lof_accum).rank(ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. UNIFIED CONFIDENCE SCORER\n",
    "# ==========================================\n",
    "def simple_anomaly_scoring(\n",
    "    df,\n",
    "    contamination_rate=0.01, \n",
    "    decay_power=1.0,  \n",
    "    sigma_margin=3.0,\n",
    "    step_size=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates a unified confidence score in [0, 1].\n",
    "    - High score: global agreement (KNN + LOF)\n",
    "    - Mid score: local-only or global-only detection\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    N = len(df)\n",
    "    \n",
    "    # Dynamic cutoff using binomial 3-sigma rule\n",
    "    mu = N * contamination_rate \n",
    "    sigma = math.sqrt(N * contamination_rate * (1 - contamination_rate))\n",
    "    limit_rank = mu + sigma_margin * sigma\n",
    "    max_tau = min(limit_rank / N, 1.0)\n",
    "    \n",
    "    # Voting\n",
    "    df['total_votes'] = 0.0\n",
    "    taus = np.arange(1.0 / N, max_tau + step_size, step_size)\n",
    "    \n",
    "    max_possible = 0.0\n",
    "    \n",
    "    print(f\"Scanning top {max_tau*100:.2f}% ({len(taus)} steps)...\")\n",
    "\n",
    "    for tau in taus:\n",
    "        weight = 1.0 / (tau ** decay_power)\n",
    "        max_possible += 2 * weight\n",
    "        \n",
    "        c_tau = max(int(np.ceil(N * tau)), 1)\n",
    "        \n",
    "        if_knn = df['knn_rank'] <= c_tau\n",
    "        if_lof = df['lof_rank'] <= c_tau\n",
    "        \n",
    "        df.loc[if_knn, 'total_votes'] += weight\n",
    "        df.loc[if_lof, 'total_votes'] += weight\n",
    "\n",
    "    df['confidence_score'] = df['total_votes'] / max_possible\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# A. Ensemble ranks and scores\n",
    "df_ensembled = generate_ensemble_ranks_pyod(\n",
    "    df,\n",
    "    k_range=range(5, 60, 1)\n",
    ")\n",
    "\n",
    "# B. Unified confidence score\n",
    "df_final = simple_anomaly_scoring(\n",
    "    df_ensembled,\n",
    "    contamination_rate=1,\n",
    "    decay_power=1.0,\n",
    "    sigma_margin=3.0\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. LOF MAGNITUDE â†’ POINT SIZE\n",
    "# ==========================================\n",
    "\n",
    "# Robust normalization of accumulated LOF score\n",
    "\n",
    "lof_raw = df_final['lof_score']\n",
    "q_low = lof_raw.quantile(0.05)\n",
    "q_high = lof_raw.quantile(0.99)\n",
    "\n",
    "df_final['lof_size_norm'] = (\n",
    "    lof_raw.clip(q_low, q_high) - q_low\n",
    ") / (q_high - q_low)\n",
    "\n",
    "# Map to marker size (area)\n",
    "min_size = 20\n",
    "max_size = 300\n",
    "gamma = 0.7\n",
    "\n",
    "df_final['marker_size'] = (\n",
    "    min_size\n",
    "    + (max_size - min_size)\n",
    "    * (df_final['lof_size_norm'] ** gamma)\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZATION\n",
    "# ==========================================\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(11, 8))\n",
    "\n",
    "# Filter weak noise\n",
    "mask_signal = df_final['confidence_score'] > 0.01\n",
    "df_plot = df_final[mask_signal].copy()\n",
    "mask_noise = ~mask_signal\n",
    "\n",
    "# Background points\n",
    "ax.scatter(\n",
    "    df_final.loc[mask_noise, 'attribute_1'],\n",
    "    df_final.loc[mask_noise, 'attribute_2'],\n",
    "    c='#e0e0e0',\n",
    "    s=15,\n",
    "    alpha=0.3,\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "# Anomalies: color = confidence, size = LOF magnitude\n",
    "sc = ax.scatter(\n",
    "    df_plot['attribute_1'],\n",
    "    df_plot['attribute_2'],\n",
    "    c=df_plot['confidence_score'],\n",
    "    cmap='plasma',\n",
    "    s=df_plot['marker_size'],\n",
    "    alpha=0.9,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.set_label(\"Anomaly Globality Index\", rotation=270, labelpad=15)\n",
    "cbar.set_ticks([0.1, 0.95])\n",
    "cbar.set_ticklabels(['Local Outlier', 'Global Outlier'])\n",
    "\n",
    "ax.set_title(\n",
    "    \"Anomaly Detection Explainability\\n\"\n",
    "    \"Color = Global Locality Index | Size = Normalized LOF Score\",\n",
    "    fontsize=14\n",
    ")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 6. TOP ANOMALIES\n",
    "# ==========================================\n",
    "\n",
    "print(\"Top 5 Anomalies:\")\n",
    "print(\n",
    "    df_final\n",
    "    .sort_values('confidence_score', ascending=False)\n",
    "    .head(5)[['attribute_1', 'attribute_2', 'confidence_score', 'lof_score']]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastLOF Env",
   "language": "python",
   "name": "fastlof-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
