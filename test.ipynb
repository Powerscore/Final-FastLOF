{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Define analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy.io import arff\n",
    "import copy\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from pyod.models.lof import LOF\n",
    "from fastlof import FastLOF\n",
    "from ranged_lof import RangedLOF\n",
    "\n",
    "global lof_auto_scores, lof_auto_time_stats, lof_brute_time_stats, fastlof_results, lof_auto_metrics\n",
    "global ranged_lof_scores, ranged_lof_time_stats, ranged_lof_metrics\n",
    "lof_auto_scores = None\n",
    "lof_auto_time_stats = None\n",
    "lof_brute_time_stats = None\n",
    "fastlof_results = None\n",
    "lof_auto_metrics = None\n",
    "ranged_lof_scores = None\n",
    "ranged_lof_time_stats = None\n",
    "ranged_lof_metrics = None\n",
    "\n",
    "\n",
    "def load_dataset(filepath, fraction=1.0):\n",
    "    \"\"\"Load dataset from various file formats (.csv, .mat, .arff).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray\n",
    "        Feature matrix\n",
    "    y : ndarray or None\n",
    "        Binary labels (1 = anomaly, 0 = normal) if available, else None\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(filepath)\n",
    "    ext = ext.lower()\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "    # Lightweight preview of the dataset (first 5 rows with column headings, where possible)\n",
    "    try:\n",
    "        if ext == '.csv':\n",
    "            preview_df = pd.read_csv(filepath, nrows=5)\n",
    "            print(\"\\n--- Dataset preview (first 5 rows) ---\")\n",
    "            print(preview_df.head())\n",
    "            print(\"--------------------------------------\")\n",
    "        elif ext == '.arff':\n",
    "            preview_data, preview_meta = arff.loadarff(filepath)\n",
    "            preview_df = pd.DataFrame(preview_data.tolist(), columns=preview_data.dtype.names)\n",
    "            print(\"\\n--- Dataset preview (first 5 rows) ---\")\n",
    "            print(preview_df.head())\n",
    "            print(\"--------------------------------------\")\n",
    "        # For .mat we will preview after loading X below\n",
    "    except Exception as e:\n",
    "        print(f\"(Preview skipped due to error: {e})\")\n",
    "\n",
    "    def _label_from_value(value):\n",
    "        \"\"\"Convert textual/numeric labels to binary anomaly flags.\"\"\"\n",
    "        if isinstance(value, (bytes, bytearray)):\n",
    "            value = value.decode('utf-8')\n",
    "        value_str = str(value).strip().strip('\"').strip(\"'\").lower()\n",
    "        if value_str in {\n",
    "            '1', 'anomaly', 'attack', 'outlier', 'abnormal', 'yes', 'true',\n",
    "            'o', 'outliers', 'anomalous', 'attack.', 'anomaly.', 'outlier.', 'abnormal.'\n",
    "        }:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    if ext == '.csv':\n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(',')\n",
    "                try:\n",
    "                    row = [float(x) for x in parts]\n",
    "                    data.append(row)\n",
    "                except ValueError:\n",
    "                    # Attempt to treat last column as label\n",
    "                    try:\n",
    "                        features = [float(x) for x in parts[:-1]]\n",
    "                        label = _label_from_value(parts[-1])\n",
    "                        data.append(features)\n",
    "                        labels.append(label)\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Could not process row (even after label handling): {line}\")\n",
    "                        continue\n",
    "        if not data:\n",
    "            raise ValueError(\"No valid rows were loaded from the CSV.\")\n",
    "        X = np.array(data, dtype=float)\n",
    "        if labels and len(labels) == len(data):\n",
    "            y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    elif ext == '.mat':\n",
    "        try:\n",
    "            mat_data = scipy.io.loadmat(filepath)\n",
    "            if 'X' in mat_data and isinstance(mat_data['X'], np.ndarray) and mat_data['X'].ndim == 2:\n",
    "                X = mat_data['X']\n",
    "            elif 'data' in mat_data and isinstance(mat_data['data'], np.ndarray) and mat_data['data'].ndim == 2:\n",
    "                X = mat_data['data']\n",
    "            else:\n",
    "                potential_X = None\n",
    "                max_size = 0\n",
    "                for key, value in mat_data.items():\n",
    "                    if isinstance(value, np.ndarray) and value.ndim == 2 and value.size > max_size and not key.startswith('__'):\n",
    "                        potential_X = value\n",
    "                        max_size = value.size\n",
    "                if potential_X is not None:\n",
    "                    X = potential_X\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find a suitable 2D data array in .mat file. Available keys: {mat_data.keys()}\")\n",
    "\n",
    "            X = X.astype(float)\n",
    "\n",
    "            # Preview first 5 rows for .mat as a DataFrame with generic column names\n",
    "            try:\n",
    "                import pandas as _pd_mat_preview\n",
    "                n_cols = X.shape[1]\n",
    "                col_names = [f\"f{i}\" for i in range(n_cols)]\n",
    "                preview_df = _pd_mat_preview.DataFrame(X[:5, :], columns=col_names)\n",
    "                print(\"\\n--- Dataset preview (first 5 rows) [from .mat X] ---\")\n",
    "                print(preview_df.head())\n",
    "                print(\"--------------------------------------\")\n",
    "            except Exception as e:\n",
    "                print(f\"(Preview for .mat skipped due to error: {e})\")\n",
    "\n",
    "            for label_key in ['y', 'Y', 'labels', 'label']:\n",
    "                if label_key in mat_data:\n",
    "                    label_arr = mat_data[label_key]\n",
    "                    label_arr = np.ravel(label_arr)\n",
    "                    if label_arr.size == X.shape[0]:\n",
    "                        y = np.array([_label_from_value(val) for val in label_arr], dtype=np.int32)\n",
    "                        break\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading .mat file '{filepath}': {e}\")\n",
    "\n",
    "    elif ext == '.arff':\n",
    "        try:\n",
    "            data, meta = arff.loadarff(filepath)\n",
    "            numeric_cols = []\n",
    "            labels = None\n",
    "            for col_name in data.dtype.names:\n",
    "                col_data = data[col_name]\n",
    "                if np.issubdtype(col_data.dtype, np.number):\n",
    "                    numeric_cols.append(col_data)\n",
    "                else:\n",
    "                    if labels is None:\n",
    "                        labels = np.array([_label_from_value(val) for val in col_data], dtype=np.int32)\n",
    "                    else:\n",
    "                        print(f\"Warning: Multiple non-numeric columns detected. Using '{col_name}' as additional label information.\")\n",
    "                        labels = np.array([_label_from_value(val) for val in col_data], dtype=np.int32)\n",
    "\n",
    "            if not numeric_cols:\n",
    "                raise ValueError(\"No numeric columns found in .arff file after filtering.\")\n",
    "\n",
    "            X = np.column_stack(numeric_cols).astype(float)\n",
    "            if labels is not None and labels.shape[0] == X.shape[0]:\n",
    "                y = labels\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading .arff file '{filepath}': {e}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}. Supported formats are .csv, .mat, .arff.\")\n",
    "\n",
    "    if X is None or X.size == 0:\n",
    "        raise ValueError(\"Loaded data is empty or could not be processed.\")\n",
    "\n",
    "    if fraction < 1.0:\n",
    "        n_samples = int(X.shape[0] * fraction)\n",
    "        X = X[:n_samples]\n",
    "        if y is not None:\n",
    "            y = y[:n_samples]\n",
    "        print(f\"Loaded {fraction*100:.0f}% of dataset: {X.shape[0]} samples\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_lof_auto(X_normalized, k, contamination, n_runs):\n",
    "    \"\"\"Run LOF (auto) multiple times and collect scores and timing statistics.\"\"\"\n",
    "    print(f\"\\nTesting LOF (auto) ({n_runs} runs)...\")\n",
    "\n",
    "    def run_lof():\n",
    "        lof = LOF(n_neighbors=k, contamination=contamination, algorithm='auto')\n",
    "        lof.fit(X_normalized)\n",
    "        return lof.decision_scores_, lof.detector_._fit_method\n",
    "\n",
    "    # Warmup run\n",
    "    print(\"  Warming up LOF (auto)...\")\n",
    "    warmup_n = min(max(k + 1, 1024), X_normalized.shape[0])\n",
    "    if warmup_n < X_normalized.shape[0]:\n",
    "        lof_wu = LOF(n_neighbors=k, contamination=contamination, algorithm='auto')\n",
    "        lof_wu.fit(X_normalized[:warmup_n])\n",
    "        _ = lof_wu.decision_scores_\n",
    "    else:\n",
    "        _ = run_lof()\n",
    "\n",
    "    # Collect scores and times\n",
    "    all_scores = []\n",
    "    times = []\n",
    "    fit_method = None\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        scores, fit_method = run_lof()\n",
    "        t_run = time.perf_counter() - t0\n",
    "        all_scores.append(scores)\n",
    "        times.append(t_run)\n",
    "\n",
    "    print(f\"Algorithm chosen by Auto: {fit_method}\")\n",
    "    avg_scores = np.mean(all_scores, axis=0)\n",
    "\n",
    "    time_stats = {\n",
    "        'avg': np.mean(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'std': np.std(times),\n",
    "        'times': times,\n",
    "        'algorithm': 'LOF auto: ' + fit_method\n",
    "    }\n",
    "\n",
    "    print(f\"LOF (auto) - avg: {time_stats['avg']:.4f}s, \"\n",
    "          f\"min: {time_stats['min']:.4f}s, \"\n",
    "          f\"max: {time_stats['max']:.4f}s, \"\n",
    "          f\"std: {time_stats['std']:.4f}s\")\n",
    "\n",
    "    return avg_scores, time_stats\n",
    "\n",
    "\n",
    "def run_lof_brute(X_normalized, k, contamination, n_runs):\n",
    "    \"\"\"Run LOF (brute) multiple times and collect scores and timing statistics.\"\"\"\n",
    "    print(f\"\\nTesting LOF (brute) ({n_runs} runs)...\")\n",
    "\n",
    "    def run_lof():\n",
    "        lof = LOF(n_neighbors=k, contamination=contamination, algorithm='brute')\n",
    "        lof.fit(X_normalized)\n",
    "        return lof.decision_scores_\n",
    "\n",
    "    # Warmup run\n",
    "    print(\"  Warming up LOF (brute)...\")\n",
    "    warmup_n = min(max(k + 1, 1024), X_normalized.shape[0])\n",
    "    if warmup_n < X_normalized.shape[0]:\n",
    "        lof_wu = LOF(n_neighbors=k, contamination=contamination, algorithm='brute')\n",
    "        lof_wu.fit(X_normalized[:warmup_n])\n",
    "        _ = lof_wu.decision_scores_\n",
    "    else:\n",
    "        _ = run_lof()\n",
    "\n",
    "    all_scores = []\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        scores = run_lof()\n",
    "        t_run = time.perf_counter() - t0\n",
    "        all_scores.append(scores)\n",
    "        times.append(t_run)\n",
    "\n",
    "    avg_scores = np.mean(all_scores, axis=0)\n",
    "\n",
    "    time_stats = {\n",
    "        'avg': np.mean(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'std': np.std(times),\n",
    "        'times': times,\n",
    "        'algorithm': 'LOF_brute'\n",
    "    }\n",
    "    \n",
    "    print(f\"LOF (brute) - avg: {time_stats['avg']:.4f}s, \"\n",
    "          f\"min: {time_stats['min']:.4f}s, \"\n",
    "          f\"max: {time_stats['max']:.4f}s, \"\n",
    "          f\"std: {time_stats['std']:.4f}s\")\n",
    "\n",
    "    return avg_scores, time_stats\n",
    "\n",
    "def run_ranged_lof(X_normalized, k, contamination, n_runs, n_neighbors_lb=None):\n",
    "    \"\"\"Run Ranged LOF multiple times and collect scores and timing statistics.\"\"\"\n",
    "    print(f\"\\nTesting Ranged LOF ({n_runs} runs)...\")\n",
    "\n",
    "    if n_neighbors_lb is None:\n",
    "        n_neighbors_lb = k\n",
    "        \n",
    "    def run_once():\n",
    "        rlof = RangedLOF(\n",
    "            n_neighbors=k,\n",
    "            n_neighbors_lb=n_neighbors_lb,\n",
    "            contamination=contamination,\n",
    "            algorithm='auto',\n",
    "        )\n",
    "        rlof.fit(X_normalized)\n",
    "        return rlof.decision_scores_\n",
    "\n",
    "    # Warmup run\n",
    "    print(\"  Warming up Ranged LOF...\")\n",
    "    _ = run_once()\n",
    "\n",
    "    all_scores = []\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        scores = run_once()\n",
    "        t_run = time.perf_counter() - t0\n",
    "        all_scores.append(scores)\n",
    "        times.append(t_run)\n",
    "\n",
    "    avg_scores = np.mean(all_scores, axis=0)\n",
    "\n",
    "    time_stats = {\n",
    "        'avg': np.mean(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'std': np.std(times),\n",
    "        'times': times,\n",
    "        'algorithm': 'RangedLOF',\n",
    "        'n_neighbors_lb': n_neighbors_lb,\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Ranged LOF - avg: {time_stats['avg']:.4f}s, \"\n",
    "        f\"min: {time_stats['min']:.4f}s, \"\n",
    "        f\"max: {time_stats['max']:.4f}s, \"\n",
    "        f\"std: {time_stats['std']:.4f}s\"\n",
    "    )\n",
    "\n",
    "    return avg_scores, time_stats\n",
    "\n",
    "    # Warmup run\n",
    "    print(\"  Warming up LOF (brute)...\")\n",
    "    warmup_n = min(max(k + 1, 1024), X_normalized.shape[0])\n",
    "    if warmup_n < X_normalized.shape[0]:\n",
    "        lof_wu = LOF(n_neighbors=k, contamination=contamination, algorithm='brute')\n",
    "        lof_wu.fit(X_normalized[:warmup_n])\n",
    "        _ = lof_wu.decision_scores_\n",
    "    else:\n",
    "        _ = run_lof()\n",
    "\n",
    "    # Collect times\n",
    "    times = []\n",
    "\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = run_lof()\n",
    "        t_run = time.perf_counter() - t0\n",
    "        times.append(t_run)\n",
    "\n",
    "    time_stats = {\n",
    "        'avg': np.mean(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'std': np.std(times),\n",
    "        'times': times,\n",
    "        'algorithm': 'LOF_brute'\n",
    "    }\n",
    "\n",
    "    print(f\"LOF (brute) - avg: {time_stats['avg']:.4f}s, \"\n",
    "          f\"min: {time_stats['min']:.4f}s, \"\n",
    "          f\"max: {time_stats['max']:.4f}s, \"\n",
    "          f\"std: {time_stats['std']:.4f}s\")\n",
    "\n",
    "    return time_stats\n",
    "\n",
    "\n",
    "def _aggregate_timing_info(timing_dicts):\n",
    "    \"\"\"Average timing dictionaries across runs, preserving structural info.\"\"\"\n",
    "    if not timing_dicts:\n",
    "        return None\n",
    "\n",
    "    phase_keys = [\n",
    "        'total', 'initialization', 'chunk_processing',\n",
    "        'lof_calculation', 'active_set_updates', 'finalization'\n",
    "    ]\n",
    "    detail_keys = ['distance_computation', 'neighbor_updates', 'self_distance_handling']\n",
    "\n",
    "    agg = {k: 0.0 for k in phase_keys}\n",
    "    agg['chunk_processing_details'] = {k: 0.0 for k in detail_keys}\n",
    "\n",
    "    for timing in timing_dicts:\n",
    "        for key in phase_keys:\n",
    "            agg[key] += timing.get(key, 0.0)\n",
    "        details = timing.get('chunk_processing_details', {})\n",
    "        for key in detail_keys:\n",
    "            agg['chunk_processing_details'][key] += details.get(key, 0.0)\n",
    "\n",
    "    count = len(timing_dicts)\n",
    "    for key in phase_keys:\n",
    "        agg[key] /= count\n",
    "    for key in detail_keys:\n",
    "        agg['chunk_processing_details'][key] /= count\n",
    "\n",
    "    # Keep a copy of the structural information from the last run (distances_computed, passes, iterations, n_chunks)\n",
    "    last = timing_dicts[-1]\n",
    "    agg['distances_computed'] = last.get('distances_computed')\n",
    "    agg['passes'] = copy.deepcopy(last.get('passes', []))\n",
    "    agg['iterations'] = copy.deepcopy(last.get('iterations', []))\n",
    "    agg['n_chunks'] = last.get('n_chunks')\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def _compute_anomaly_metrics(y_true, scores, contamination):\n",
    "    \"\"\"Compute ROC/PR metrics given ground-truth labels.\"\"\"\n",
    "    if y_true is None:\n",
    "        return None\n",
    "\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    if y_true.shape[0] != scores.shape[0]:\n",
    "        raise ValueError(\"Label array and scores must have the same length for metric evaluation.\")\n",
    "\n",
    "    # Check for only one class - crucial for metric calculation\n",
    "    if np.unique(y_true).size < 2:\n",
    "        print(\"Warning: Only one class present in labels. Skipping metric computation.\")\n",
    "        return None\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, scores)\n",
    "    pr_auc = average_precision_score(y_true, scores)\n",
    "\n",
    "    k = max(1, int(len(y_true) * contamination))\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    top_k_idx = sorted_idx[:k]\n",
    "    precision_at_k = np.sum(y_true[top_k_idx]) / k\n",
    "    recall_at_k = np.sum(y_true[top_k_idx]) / max(1, np.sum(y_true))\n",
    "\n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'k_top': k\n",
    "    }\n",
    "\n",
    "\n",
    "def run_fastlof_multiple_chunks(X_normalized, k, contamination, n_runs, dataset_size,\n",
    "                                 min_chunk_size, max_chunk_size, chunk_interval, threshold=1.2,\n",
    "                                 y_true=None):\n",
    "    \"\"\"Run FastLOF with multiple chunk sizes.\"\"\"\n",
    "    print(f\"\\nTesting FastLOF with multiple chunk sizes ({n_runs} runs each)...\")\n",
    "\n",
    "    # Warmup run\n",
    "    print(\"  Warming up FastLOF...\")\n",
    "    warmup_n = min(max(k + 1, 1024), X_normalized.shape[0])\n",
    "    fastlof = FastLOF(n_neighbors=k, contamination=contamination, threshold=threshold)\n",
    "    if warmup_n < X_normalized.shape[0]:\n",
    "        fastlof.fit(X_normalized[:warmup_n])\n",
    "    else:\n",
    "        fastlof.fit(X_normalized)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    chunk_interval = int(chunk_interval)\n",
    "    if chunk_interval <= 0:\n",
    "        raise ValueError(\"chunk_interval must be a positive integer\")\n",
    "\n",
    "    max_chunk_size = min(int(max_chunk_size), dataset_size)\n",
    "    min_chunk_size = max(int(min_chunk_size), k + 1)\n",
    "\n",
    "    if min_chunk_size > max_chunk_size:\n",
    "        # Raise an error/warning as requested for per dataset handling.\n",
    "        raise ValueError(f\"min_chunk_size ({min_chunk_size}) must be <= max_chunk_size ({max_chunk_size}) (after min clip to k+1). Adjust parameters.\")\n",
    "\n",
    "    chunk_sizes = list(range(max_chunk_size, min_chunk_size - 1, -chunk_interval))\n",
    "    if not chunk_sizes or chunk_sizes[-1] != min_chunk_size:\n",
    "        chunk_sizes.append(min_chunk_size)\n",
    "\n",
    "    # Use set to remove duplicates and sort by size (descending for plotting consistency later)\n",
    "    chunk_sizes = sorted(list(set(chunk_sizes)), reverse=True)\n",
    "\n",
    "    print(f\"  Testing {len(chunk_sizes)} different chunk sizes...\")\n",
    "    \n",
    "    # Store FastLOF results and times for final output\n",
    "    for idx, chunk_size in enumerate(chunk_sizes):\n",
    "        chunk_count = int(math.ceil(dataset_size / chunk_size))\n",
    "        print(f\"  [{idx+1}/{len(chunk_sizes)}] Chunk size: {chunk_size}, Chunk count: {chunk_count}\")\n",
    "\n",
    "        all_scores = []\n",
    "        times = []\n",
    "        timing_runs = []\n",
    "\n",
    "        for run_idx in range(n_runs):\n",
    "            def run_fastlof():\n",
    "                fastlof = FastLOF(n_neighbors=k, contamination=contamination,\n",
    "                                  chunk_size=chunk_size, threshold=threshold)\n",
    "                fastlof.fit(X_normalized)\n",
    "                return fastlof.decision_scores_, copy.deepcopy(fastlof.timing_)\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            scores, timing_info = run_fastlof()\n",
    "            t_run = time.perf_counter() - t0\n",
    "            all_scores.append(scores)\n",
    "            times.append(t_run)\n",
    "            timing_runs.append(timing_info)\n",
    "\n",
    "        avg_scores = np.mean(all_scores, axis=0)\n",
    "\n",
    "        time_stats = {\n",
    "            'avg': np.mean(times),\n",
    "            'min': np.min(times),\n",
    "            'max': np.max(times),\n",
    "            'std': np.std(times),\n",
    "            'times': times,\n",
    "            'algorithm': 'FastLOF',\n",
    "            'chunk_size': chunk_size\n",
    "        }\n",
    "\n",
    "        timing_summary = _aggregate_timing_info(timing_runs)\n",
    "\n",
    "        metrics = None\n",
    "        if y_true is not None:\n",
    "            metrics = _compute_anomaly_metrics(y_true, avg_scores, contamination)\n",
    "\n",
    "        results.append({\n",
    "            'chunk_size': chunk_size,\n",
    "            'chunk_count': chunk_count,\n",
    "            'avg_scores': avg_scores,\n",
    "            'time_stats': time_stats,\n",
    "            'timing': timing_summary,\n",
    "            'metrics': metrics\n",
    "        })\n",
    "\n",
    "        print(f\"    avg: {time_stats['avg']:.4f}s, min: {time_stats['min']:.4f}s, \"\n",
    "              f\"max: {time_stats['max']:.4f}s\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_timing_csv(lof_auto_time_stats, lof_brute_time_stats, fastlof_results,\n",
    "                    dataset_filepath, k, contamination, n_runs, threshold,\n",
    "                    ranged_lof_time_stats=None):\n",
    "    \"\"\"Saves final timing results to a CSV file.\n",
    "\n",
    "    Output structure (relative to project root):\n",
    "    results/<dataset_name>/k{k}_t{threshold}/...\n",
    "    where <dataset_name> is the dataset filename without extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    timing_data = []\n",
    "\n",
    "    # 1. LOF Auto\n",
    "    if lof_auto_time_stats:\n",
    "        timing_data.append({\n",
    "            'Algorithm': 'LOF_auto',\n",
    "            'k': k,\n",
    "            'Contamination': contamination,\n",
    "            'N_Runs': n_runs,\n",
    "            'Chunk_Size': 'N/A',\n",
    "            'Avg_Time_s': lof_auto_time_stats['avg'],\n",
    "            'Min_Time_s': lof_auto_time_stats['min'],\n",
    "            'Max_Time_s': lof_auto_time_stats['max'],\n",
    "            'Std_Dev_s': lof_auto_time_stats['std']\n",
    "        })\n",
    "\n",
    "    # 2. LOF Brute\n",
    "    if lof_brute_time_stats:\n",
    "        timing_data.append({\n",
    "            'Algorithm': 'LOF_brute',\n",
    "            'k': k,\n",
    "            'Contamination': contamination,\n",
    "            'N_Runs': n_runs,\n",
    "            'Chunk_Size': 'N/A',\n",
    "            'Avg_Time_s': lof_brute_time_stats['avg'],\n",
    "            'Min_Time_s': lof_brute_time_stats['min'],\n",
    "            'Max_Time_s': lof_brute_time_stats['max'],\n",
    "            'Std_Dev_s': lof_brute_time_stats['std']\n",
    "        })\n",
    "\n",
    "    # 2b. Ranged LOF (optional)\n",
    "    if ranged_lof_time_stats:\n",
    "        timing_data.append({\n",
    "            'Algorithm': 'RangedLOF',\n",
    "            'k': k,\n",
    "            'Contamination': contamination,\n",
    "            'N_Runs': n_runs,\n",
    "            'Chunk_Size': 'N/A',\n",
    "            'Avg_Time_s': ranged_lof_time_stats['avg'],\n",
    "            'Min_Time_s': ranged_lof_time_stats['min'],\n",
    "            'Max_Time_s': ranged_lof_time_stats['max'],\n",
    "            'Std_Dev_s': ranged_lof_time_stats['std']\n",
    "        })\n",
    "\n",
    "    # 3. FastLOF Results\n",
    "    for result in fastlof_results:\n",
    "        time_stats = result['time_stats']\n",
    "        timing_data.append({\n",
    "            'Algorithm': 'FastLOF',\n",
    "            'k': k,\n",
    "            'Contamination': contamination,\n",
    "            'N_Runs': n_runs,\n",
    "            'Chunk_Size': result['chunk_size'],\n",
    "            'Avg_Time_s': time_stats['avg'],\n",
    "            'Min_Time_s': time_stats['min'],\n",
    "            'Max_Time_s': time_stats['max'],\n",
    "            'Std_Dev_s': time_stats['std']\n",
    "        })\n",
    "\n",
    "    if not timing_data:\n",
    "        print(\"Warning: No timing data available to save.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(timing_data)\n",
    "\n",
    "    # Build results directory from current working directory: results/<dataset_name>/k{k}_t{threshold}\n",
    "    dataset_name = os.path.splitext(os.path.basename(dataset_filepath))[0]\n",
    "    project_root = os.getcwd()\n",
    "    results_base = os.path.join(project_root, 'results', dataset_name)\n",
    "    k_threshold_folder = os.path.join(results_base, f\"k{k}_t{threshold}\")\n",
    "    \n",
    "    os.makedirs(k_threshold_folder, exist_ok=True)\n",
    "    \n",
    "    csv_filename = f\"timing_results_{dataset_name}.csv\"\n",
    "    csv_path = os.path.join(k_threshold_folder, csv_filename)\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n Timing results successfully saved to: '{csv_path}'\")\n",
    "    return csv_path\n",
    "\n",
    "\n",
    "def create_plots(lof_auto_scores, lof_auto_time_stats, lof_brute_time_stats, fastlof_results,\n",
    "                 n_runs, threshold, fraction, filepath, k,\n",
    "                 lof_auto_metrics=None,\n",
    "                 ranged_lof_time_stats=None,\n",
    "                 ranged_lof_metrics=None,\n",
    "                 ranged_lof_scores=None):\n",
    "    \"\"\"Create comparison plots and save to the results folder.\n",
    "\n",
    "    Output structure (relative to project root):\n",
    "    results/<dataset_name>/k{k}_t{threshold}/...\n",
    "    where <dataset_name> is the dataset filename without extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build results directory from current working directory: results/<dataset_name>/k{k}_t{threshold}\n",
    "    dataset_name = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    project_root = os.getcwd()\n",
    "    results_base = os.path.join(project_root, 'results', dataset_name)\n",
    "    k_threshold_folder = os.path.join(results_base, f\"k{k}_t{threshold}\")\n",
    "    os.makedirs(k_threshold_folder, exist_ok=True)\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if lof_auto_scores is None or fastlof_results is None or not fastlof_results:\n",
    "        print(\"Skipping plot generation: Missing LOF auto scores or FastLOF results.\")\n",
    "        return []\n",
    "\n",
    "    metrics_available = (\n",
    "        (lof_auto_metrics is not None or ranged_lof_metrics is not None)\n",
    "        and any(res.get('metrics') for res in fastlof_results)\n",
    "    )\n",
    "    num_plots = 4 if metrics_available else 3\n",
    "    fig = plt.figure(figsize=(6 * num_plots, 5))\n",
    "\n",
    "    fig.suptitle(\n",
    "        f'FastLOF vs LOF Comparison (dataset={os.path.basename(filepath)}, runs={n_runs}, '\n",
    "        f'FastLOF threshold={threshold}, k={k}, fraction={fraction})',\n",
    "        fontsize=14, fontweight='bold', y=0.98\n",
    "    )\n",
    "\n",
    "    ax1 = plt.subplot(1, num_plots, 1)\n",
    "\n",
    "    chunk_sizes = []\n",
    "    correlations = []\n",
    "\n",
    "    for result in fastlof_results:\n",
    "        chunk_size = result['chunk_size']\n",
    "        fastlof_scores = result['avg_scores']\n",
    "\n",
    "        correlation = np.corrcoef(lof_auto_scores, fastlof_scores)[0, 1]\n",
    "        chunk_sizes.append(chunk_size)\n",
    "        correlations.append(correlation)\n",
    "\n",
    "    # Sort by chunk size for consistent plotting\n",
    "    sorted_indices = np.argsort(chunk_sizes)\n",
    "    chunk_sizes_sorted = np.array(chunk_sizes)[sorted_indices]\n",
    "    correlations_sorted = np.array(correlations)[sorted_indices]\n",
    "\n",
    "    ax1.plot(chunk_sizes_sorted, correlations_sorted, 'o-', linewidth=2, markersize=6)\n",
    "    ax1.set_xlabel('Chunk Size', fontsize=12)\n",
    "    ax1.set_ylabel('Correlation with LOF (auto)', fontsize=12)\n",
    "    ax1.set_title('FastLOF vs LOF (auto) Correlation', fontsize=13)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(chunk_sizes_sorted) <= 20: \n",
    "        ax1.set_xticks(chunk_sizes_sorted)\n",
    "        ax1.set_xticklabels([str(int(cs)) for cs in chunk_sizes_sorted], rotation=45, ha='right', fontsize=9)\n",
    "    else:\n",
    "        # Simplified tick generation for many points\n",
    "        min_cs = chunk_sizes_sorted.min()\n",
    "        max_cs = chunk_sizes_sorted.max()\n",
    "        ticks = np.linspace(min_cs, max_cs, min(10, len(chunk_sizes_sorted)))\n",
    "        ax1.set_xticks(ticks)\n",
    "        ax1.set_xticklabels([str(int(t)) for t in ticks], rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "\n",
    "    ax2 = plt.subplot(1, num_plots, 2)\n",
    "\n",
    "    names = ['LOF (auto)', 'LOF (brute)']\n",
    "    colors = ['blue', 'green']\n",
    "\n",
    "    times_avg = [lof_auto_time_stats['avg'], lof_brute_time_stats['avg']]\n",
    "    times_min = [lof_auto_time_stats['min'], lof_brute_time_stats['min']]\n",
    "    times_max = [lof_auto_time_stats['max'], lof_brute_time_stats['max']]\n",
    "\n",
    "    if ranged_lof_time_stats is not None:\n",
    "        names.append('Ranged LOF')\n",
    "        colors.append('orange')\n",
    "        times_avg.append(ranged_lof_time_stats['avg'])\n",
    "        times_min.append(ranged_lof_time_stats['min'])\n",
    "        times_max.append(ranged_lof_time_stats['max'])\n",
    "\n",
    "    x_pos = np.arange(len(names))\n",
    "\n",
    "    error_lower = [max(0, avg - tmin) for avg, tmin in zip(times_avg, times_min)]\n",
    "    error_upper = [tmax - avg for avg, tmax in zip(times_avg, times_max)]\n",
    "\n",
    "    bars = ax2.bar(x_pos, times_avg, color=colors, alpha=0.7,\n",
    "                     yerr=[error_lower, error_upper], capsize=8,\n",
    "                     error_kw={'elinewidth': 2, 'capthick': 2})\n",
    "\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(names, fontsize=11)\n",
    "    ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Runtime Comparison (LOF auto vs brute)', fontsize=13)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    for i, (bar, avg, tmin, tmax, err_upper) in enumerate(zip(bars, times_avg, times_min, times_max, error_upper)):\n",
    "        height = bar.get_height()\n",
    "        text_y = height + err_upper + height * 0.05\n",
    "        current_top = ax2.get_ylim()[1]\n",
    "        if height > 0 and height < (current_top * 0.1): # Avoid text overlap for very small bars\n",
    "             text_y = current_top * 0.1\n",
    "        \n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                 f'avg: {avg:.3f}s\\nmin: {tmin:.3f}s\\nmax: {tmax:.3f}s',\n",
    "                 ha='center', va='bottom', fontsize=9,\n",
    "                 bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.8))\n",
    "\n",
    "    current_top = ax2.get_ylim()[1]\n",
    "    ax2.set_ylim(bottom=0, top=current_top * 1.3)\n",
    "\n",
    "    ax3 = plt.subplot(1, num_plots, 3)\n",
    "\n",
    "    chunk_sizes = []\n",
    "    times_avg = []\n",
    "    times_min = []\n",
    "    times_max = []\n",
    "\n",
    "    for result in fastlof_results:\n",
    "        chunk_sizes.append(result['chunk_size'])\n",
    "        time_stats = result['time_stats']\n",
    "        times_avg.append(time_stats['avg'])\n",
    "        times_min.append(time_stats['min'])\n",
    "        times_max.append(time_stats['max'])\n",
    "\n",
    "    # Sort by chunk size for better visualization\n",
    "    sorted_indices = np.argsort(chunk_sizes)\n",
    "    chunk_sizes = [chunk_sizes[i] for i in sorted_indices]\n",
    "    times_avg = [times_avg[i] for i in sorted_indices]\n",
    "    times_min = [times_min[i] for i in sorted_indices]\n",
    "    times_max = [times_max[i] for i in sorted_indices]\n",
    "\n",
    "    x_pos = np.arange(len(chunk_sizes))\n",
    "\n",
    "    error_lower = [max(0, avg - tmin) for avg, tmin in zip(times_avg, times_min)]\n",
    "    error_upper = [tmax - avg for avg, tmax in zip(times_avg, times_max)]\n",
    "\n",
    "    bars = ax3.bar(x_pos, times_avg, alpha=0.7, color='red',\n",
    "                     yerr=[error_lower, error_upper], capsize=3,\n",
    "                     error_kw={'elinewidth': 1.5, 'capthick': 1.5})\n",
    "\n",
    "    lof_auto_avg = lof_auto_time_stats['avg'] if lof_auto_time_stats else 0\n",
    "    lof_brute_avg = lof_brute_time_stats['avg'] if lof_brute_time_stats else 0\n",
    "    ranged_lof_avg = ranged_lof_time_stats['avg'] if ranged_lof_time_stats else 0\n",
    "\n",
    "    if lof_auto_time_stats:\n",
    "        ax3.axhline(y=lof_auto_avg, color='blue', linestyle='--', linewidth=2,\n",
    "                    label=f'LOF (auto) avg: {lof_auto_avg:.3f}s', alpha=0.8)\n",
    "    if lof_brute_time_stats:\n",
    "        ax3.axhline(y=lof_brute_avg, color='green', linestyle='--', linewidth=2,\n",
    "                    label=f'LOF (brute) avg: {lof_brute_avg:.3f}s', alpha=0.8)\n",
    "    if ranged_lof_time_stats:\n",
    "        ax3.axhline(y=ranged_lof_avg, color='orange', linestyle='--', linewidth=2,\n",
    "                    label=f'Ranged LOF avg: {ranged_lof_avg:.3f}s', alpha=0.8)\n",
    "\n",
    "    ax3.set_xticks(x_pos)\n",
    "    # Only label a subset of x-ticks if there are too many\n",
    "    if len(chunk_sizes) > 20:\n",
    "        step = len(chunk_sizes) // 10\n",
    "        ax3.set_xticks(x_pos[::step])\n",
    "        ax3.set_xticklabels([str(cs) for cs in chunk_sizes[::step]], rotation=45, ha='right', fontsize=8)\n",
    "    else:\n",
    "        ax3.set_xticklabels([str(cs) for cs in chunk_sizes], rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "    ax3.set_xlabel('Chunk Size', fontsize=12)\n",
    "    ax3.set_ylabel('Time (seconds)', fontsize=12)\n",
    "    ax3.set_title('FastLOF Runtime by Chunk Size', fontsize=13)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.legend(loc='best', fontsize=9)\n",
    "\n",
    "    saved_paths = []\n",
    "    \n",
    "    if metrics_available:\n",
    "        ax4 = plt.subplot(1, num_plots, 4)\n",
    "        auc_sizes = []\n",
    "        auc_values = []\n",
    "        for result in fastlof_results:\n",
    "            metrics = result.get('metrics')\n",
    "            if metrics and metrics.get('roc_auc') is not None:\n",
    "                auc_sizes.append(result['chunk_size'])\n",
    "                auc_values.append(metrics['roc_auc'])\n",
    "\n",
    "        if auc_sizes:\n",
    "            sorted_auc_idx = np.argsort(auc_sizes)\n",
    "            auc_sizes_sorted = np.array(auc_sizes)[sorted_auc_idx]\n",
    "            auc_values_sorted = np.array(auc_values)[sorted_auc_idx]\n",
    "\n",
    "            ax4.plot(auc_sizes_sorted, auc_values_sorted, 'o-', linewidth=2, markersize=6,\n",
    "                      label='FastLOF ROC AUC')\n",
    "\n",
    "            if lof_auto_metrics is not None and lof_auto_metrics.get('roc_auc') is not None:\n",
    "                baseline_auc = lof_auto_metrics['roc_auc']\n",
    "                ax4.axhline(y=baseline_auc, color='purple', linestyle='--', linewidth=2,\n",
    "                             label=f'LOF (auto) ROC AUC: {baseline_auc:.3f}', alpha=0.8)\n",
    "\n",
    "            if ranged_lof_metrics is not None and ranged_lof_metrics.get('roc_auc') is not None:\n",
    "                ranged_auc = ranged_lof_metrics['roc_auc']\n",
    "                ax4.axhline(y=ranged_auc, color='orange', linestyle='-.', linewidth=2,\n",
    "                             label=f'Ranged LOF ROC AUC: {ranged_auc:.3f}', alpha=0.8)\n",
    "\n",
    "            ax4.set_xlabel('Chunk Size', fontsize=12)\n",
    "            ax4.set_ylabel('ROC AUC', fontsize=12)\n",
    "            ax4.set_title('FastLOF ROC AUC by Chunk Size', fontsize=13)\n",
    "            ax4.set_ylim(0.0, 1.05)\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            ax4.legend(loc='best', fontsize=9)\n",
    "            \n",
    "            if len(auc_sizes_sorted) <= 20: \n",
    "                ax4.set_xticks(auc_sizes_sorted)\n",
    "                ax4.set_xticklabels([str(int(cs)) for cs in auc_sizes_sorted], rotation=45, ha='right', fontsize=9)\n",
    "            else:\n",
    "                min_cs = auc_sizes_sorted.min()\n",
    "                max_cs = auc_sizes_sorted.max()\n",
    "                ticks = np.linspace(min_cs, max_cs, min(10, len(auc_sizes_sorted)))\n",
    "                ax4.set_xticks(ticks)\n",
    "                ax4.set_xticklabels([str(int(t)) for t in ticks], rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    comparison_path = os.path.join(k_threshold_folder, f\"comparison_plots_{n_runs}.png\")\n",
    "    fig.savefig(comparison_path, dpi=300, bbox_inches=\"tight\")\n",
    "    # Also display the figure in the notebook\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    saved_paths.append(comparison_path)\n",
    "\n",
    "    sorted_results = sorted(fastlof_results, key=lambda r: r['chunk_size']) if fastlof_results else []\n",
    "\n",
    "    if sorted_results:\n",
    "        n_results = len(sorted_results)\n",
    "        n_cols = min(3, n_results)\n",
    "        n_rows = int(np.ceil(n_results / n_cols))\n",
    "        scatter_fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4.5 * n_rows), squeeze=False)\n",
    "\n",
    "        lof_min = np.min(lof_auto_scores)\n",
    "        lof_max = np.max(lof_auto_scores)\n",
    "\n",
    "        for idx, result in enumerate(sorted_results):\n",
    "            row = idx // n_cols\n",
    "            col = idx % n_cols\n",
    "            ax = axes[row][col]\n",
    "\n",
    "            fast_scores = result['avg_scores']\n",
    "            # Filter out extreme scores (>100) for better scatter visualization, consistent with original logic\n",
    "            mask = (lof_auto_scores < 100) & (fast_scores < 100) \n",
    "\n",
    "            if not np.any(mask):\n",
    "                ax.text(0.5, 0.5, 'No points with scores < 100',\n",
    "                          transform=ax.transAxes, ha='center', va='center', fontsize=12)\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "\n",
    "            lof_filtered = lof_auto_scores[mask]\n",
    "            fast_filtered = fast_scores[mask]\n",
    "\n",
    "            correlation = np.corrcoef(lof_filtered, fast_filtered)[0, 1] if np.shape(lof_filtered)[0] > 1 else np.nan\n",
    "\n",
    "            combined_min = min(np.min(lof_filtered), np.min(fast_filtered))\n",
    "            combined_max = max(np.max(lof_filtered), np.max(fast_filtered))\n",
    "\n",
    "            ax.scatter(lof_filtered, fast_filtered, s=8, alpha=0.5, label='FastLOF vs LOF (auto)', color='teal')\n",
    "            ax.plot([combined_min, combined_max], [combined_min, combined_max],\n",
    "                      linestyle='--', color='red', linewidth=1.2, label='y = x')\n",
    "\n",
    "            ax.set_xlabel('LOF (auto) score', fontsize=11)\n",
    "            ax.set_ylabel('FastLOF score', fontsize=11)\n",
    "            ax.set_title(f'Chunk size {result[\"chunk_size\"]} (count {result[\"chunk_count\"]})\\n'\n",
    "                          f'Correlation: {correlation:.4f}', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(combined_min, combined_max)\n",
    "            ax.set_ylim(combined_min, combined_max)\n",
    "            ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        total_axes = n_rows * n_cols\n",
    "        for idx in range(len(sorted_results), total_axes):\n",
    "            row = idx // n_cols\n",
    "            col = idx % n_cols\n",
    "            axes[row][col].axis('off')\n",
    "\n",
    "        scatter_fig.suptitle(f'FastLOF vs LOF (auto) Score Correlations\\n(dataset={os.path.basename(filepath)}, runs={n_runs}, threshold={threshold}, fraction={fraction})',\n",
    "                              fontsize=15, fontweight='bold', y=0.995)\n",
    "        scatter_fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        scatter_path = os.path.join(k_threshold_folder, f\"score_scatter_{n_runs}.png\")\n",
    "        scatter_fig.savefig(scatter_path, dpi=300, bbox_inches=\"tight\")\n",
    "        # Also display the scatter figure in the notebook\n",
    "        plt.show()\n",
    "        plt.close(scatter_fig)\n",
    "        saved_paths.append(scatter_path)\n",
    "\n",
    "    timing_results = [result for result in sorted_results if result.get('timing')]\n",
    "    if timing_results:\n",
    "        timing_cols = min(3, len(timing_results))\n",
    "        timing_rows = int(np.ceil(len(timing_results) / timing_cols))\n",
    "        timing_fig, timing_axes = plt.subplots(timing_rows, timing_cols,\n",
    "                                               figsize=(6 * timing_cols, 5 * timing_rows),\n",
    "                                               squeeze=False)\n",
    "\n",
    "        axes_flat = timing_axes.flatten()\n",
    "        for idx, result in enumerate(timing_results):\n",
    "            ax = axes_flat[idx]\n",
    "            timing = result['timing'] or {}\n",
    "            details = timing.get('chunk_processing_details', {})\n",
    "            distance_time = details.get('distance_computation', 0.0)\n",
    "            neighbor_time = details.get('neighbor_updates', 0.0)\n",
    "            lof_time = timing.get('lof_calculation', 0.0)\n",
    "            other_time = (\n",
    "                timing.get('initialization', 0.0) +\n",
    "                details.get('self_distance_handling', 0.0) +\n",
    "                timing.get('active_set_updates', 0.0) +\n",
    "                timing.get('finalization', 0.0)\n",
    "            )\n",
    "\n",
    "            values = [distance_time, neighbor_time, lof_time, other_time]\n",
    "            labels = ['Distance', 'Neighbor', 'LOF calc', 'Others']\n",
    "            total_time = timing.get('total', sum(values))\n",
    "\n",
    "            if total_time <= 0 or np.isclose(sum(values), 0.0):\n",
    "                ax.text(0.5, 0.5, 'No timing data', ha='center', va='center', fontsize=11)\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            else:\n",
    "                total_time = sum(values)\n",
    "\n",
    "            def autopct_func(pct):\n",
    "                if total_time <= 0: return ''\n",
    "                value = pct * total_time / 100.0\n",
    "                return f\"{value:.2f}s\\n({pct:.1f}%)\"\n",
    "\n",
    "            wedges, texts, autotexts = ax.pie(\n",
    "                values,\n",
    "                labels=labels,\n",
    "                autopct=autopct_func,\n",
    "                startangle=90,\n",
    "                textprops={'fontsize': 8}\n",
    "            )\n",
    "            ax.axis('equal')\n",
    "            distances_info = timing.get('distances_computed', 'N/A')\n",
    "            ax.text(0.5, 1.15, f\"Distances computed: {distances_info}\",\n",
    "                     transform=ax.transAxes, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "            ax.set_title(\n",
    "                f\"Chunk size {result['chunk_size']} (count {result['chunk_count']})\\n\"\n",
    "                f\"Total time: {total_time:.2f}s\",\n",
    "                fontsize=11\n",
    "            )\n",
    "\n",
    "        for idx in range(len(timing_results), len(axes_flat)):\n",
    "            axes_flat[idx].axis('off')\n",
    "\n",
    "        timing_fig.suptitle(f'FastLOF Timing Breakdown (dataset={os.path.basename(filepath)})', fontsize=15, fontweight='bold', y=0.995)\n",
    "        timing_fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        timing_path = os.path.join(k_threshold_folder, f\"timing_breakdown_{n_runs}.png\")\n",
    "        timing_fig.savefig(timing_path, dpi=300, bbox_inches=\"tight\")\n",
    "        # Also display the timing breakdown figure in the notebook\n",
    "        plt.show()\n",
    "        plt.close(timing_fig)\n",
    "        saved_paths.append(timing_path)\n",
    "        \n",
    "    # AUC plot (separate file) - Only if metrics are available\n",
    "    if metrics_available:\n",
    "        auc_sizes = []\n",
    "        auc_values = []\n",
    "        for result in fastlof_results:\n",
    "            metrics = result.get('metrics')\n",
    "            if metrics and metrics.get('roc_auc') is not None:\n",
    "                auc_sizes.append(result['chunk_size'])\n",
    "                auc_values.append(metrics['roc_auc'])\n",
    "\n",
    "        if auc_sizes:\n",
    "            auc_fig, auc_ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            sorted_auc_idx = np.argsort(auc_sizes)\n",
    "            auc_sizes_sorted = np.array(auc_sizes)[sorted_auc_idx]\n",
    "            auc_values_sorted = np.array(auc_values)[sorted_auc_idx]\n",
    "\n",
    "            auc_ax.plot(auc_sizes_sorted, auc_values_sorted, 'o-', linewidth=2, markersize=8,\n",
    "                        label='FastLOF ROC AUC', color='blue')\n",
    "\n",
    "            if lof_auto_metrics is not None and lof_auto_metrics.get('roc_auc') is not None:\n",
    "                baseline_auc = lof_auto_metrics['roc_auc']\n",
    "                auc_ax.axhline(y=baseline_auc, color='purple', linestyle='--', linewidth=2,\n",
    "                                  label=f'LOF (auto) ROC AUC: {baseline_auc:.3f}', alpha=0.8)\n",
    "\n",
    "            if ranged_lof_metrics is not None and ranged_lof_metrics.get('roc_auc') is not None:\n",
    "                ranged_auc = ranged_lof_metrics['roc_auc']\n",
    "                auc_ax.axhline(y=ranged_auc, color='orange', linestyle='-.', linewidth=2,\n",
    "                                  label=f'Ranged LOF ROC AUC: {ranged_auc:.3f}', alpha=0.8)\n",
    "\n",
    "            auc_ax.set_xlabel('Chunk Size', fontsize=12)\n",
    "            auc_ax.set_ylabel('ROC AUC', fontsize=12)\n",
    "            auc_ax.set_title(f'FastLOF ROC AUC by Chunk Size\\n(dataset={os.path.basename(filepath)}, runs={n_runs}, threshold={threshold}, fraction={fraction})', \n",
    "                              fontsize=13, fontweight='bold')\n",
    "            auc_ax.set_ylim(0.0, 1.05)\n",
    "            auc_ax.grid(True, alpha=0.3)\n",
    "            auc_ax.legend(loc='best', fontsize=10)\n",
    "            \n",
    "            if len(auc_sizes_sorted) <= 20:\n",
    "                auc_ax.set_xticks(auc_sizes_sorted)\n",
    "                auc_ax.set_xticklabels([str(int(cs)) for cs in auc_sizes_sorted], rotation=45, ha='right', fontsize=9)\n",
    "            else:\n",
    "                min_cs = auc_sizes_sorted.min()\n",
    "                max_cs = auc_sizes_sorted.max()\n",
    "                ticks = np.linspace(min_cs, max_cs, min(10, len(auc_sizes_sorted)))\n",
    "                auc_ax.set_xticks(ticks)\n",
    "                auc_ax.set_xticklabels([str(int(t)) for t in ticks], rotation=45, ha='right', fontsize=9)\n",
    "            \n",
    "            auc_fig.tight_layout()\n",
    "            auc_path = os.path.join(k_threshold_folder, f\"auc_plot_{n_runs}.png\")\n",
    "            auc_fig.savefig(auc_path, dpi=300, bbox_inches=\"tight\")\n",
    "            # Also display the AUC figure in the notebook\n",
    "            plt.show()\n",
    "            plt.close(auc_fig)\n",
    "            saved_paths.append(auc_path)\n",
    "\n",
    "\n",
    "    # LOF vs Ranged LOF scatter plot (if Ranged LOF scores are available)\n",
    "    if ranged_lof_scores is not None:\n",
    "        try:\n",
    "            lof_scores = np.asarray(lof_auto_scores)\n",
    "            rlof_scores = np.asarray(ranged_lof_scores)\n",
    "            if lof_scores.shape[0] == rlof_scores.shape[0]:\n",
    "                # Optionally filter extreme scores for clearer visualization\n",
    "                mask = (lof_scores < 100) & (rlof_scores < 100)\n",
    "                if not np.any(mask):\n",
    "                    mask = np.ones_like(lof_scores, dtype=bool)\n",
    "\n",
    "                lof_f = lof_scores[mask]\n",
    "                rlof_f = rlof_scores[mask]\n",
    "\n",
    "                corr = np.corrcoef(lof_f, rlof_f)[0, 1] if lof_f.size > 1 else np.nan\n",
    "                scatter_lr_fig, scatter_lr_ax = plt.subplots(figsize=(6, 5))\n",
    "                scatter_lr_ax.scatter(lof_f, rlof_f, s=8, alpha=0.5, color='darkorange')\n",
    "\n",
    "                combined_min = min(np.min(lof_f), np.min(rlof_f))\n",
    "                combined_max = max(np.max(lof_f), np.max(rlof_f))\n",
    "                scatter_lr_ax.plot([combined_min, combined_max], [combined_min, combined_max],\n",
    "                                   linestyle='--', color='red', linewidth=1.2, label='y = x')\n",
    "\n",
    "                scatter_lr_ax.set_xlabel('LOF (auto) score', fontsize=11)\n",
    "                scatter_lr_ax.set_ylabel('Ranged LOF score', fontsize=11)\n",
    "                scatter_lr_ax.set_title(\n",
    "                    f'LOF (auto) vs Ranged LOF scores\\n(correlation={corr:.4f})', fontsize=12\n",
    "                )\n",
    "                scatter_lr_ax.grid(True, alpha=0.3)\n",
    "                scatter_lr_ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "                lof_rlof_scatter_path = os.path.join(k_threshold_folder, f\"lof_vs_rangedlof_scatter_{n_runs}.png\")\n",
    "                scatter_lr_fig.savefig(lof_rlof_scatter_path, dpi=300, bbox_inches=\"tight\")\n",
    "                plt.show()\n",
    "                plt.close(scatter_lr_fig)\n",
    "                saved_paths.append(lof_rlof_scatter_path)\n",
    "            else:\n",
    "                print(\"Warning: Cannot plot LOF vs Ranged LOF scatter: length mismatch.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to create LOF vs Ranged LOF scatter plot: {e}\")\n",
    "\n",
    "    if saved_paths:\n",
    "        print(\"\\n Plots saved to \" + \", \".join(f\"'{path}'\" for path in saved_paths))\n",
    "        \n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def find_best_k_lof_auto(X_normalized, y, k_values, contamination, n_runs):\n",
    "    \"\"\"Search over multiple k values using LOF (auto) and select the best ROC AUC.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_k : int\n",
    "        k with highest ROC AUC (ties broken by smallest k).\n",
    "    results_by_k : dict\n",
    "        Mapping k -> dict with 'scores', 'time_stats', 'metrics'.\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        raise ValueError(\"find_best_k_lof_auto requires labels y to compute AUC.\")\n",
    "\n",
    "    results_by_k = {}\n",
    "    best_k = None\n",
    "    best_auc = -np.inf\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"\\n=== Evaluating LOF (auto) for k={k} ===\")\n",
    "        scores, time_stats = run_lof_auto(X_normalized, k, contamination, n_runs)\n",
    "        metrics = _compute_anomaly_metrics(y, scores, contamination)\n",
    "        if metrics is None or metrics.get('roc_auc') is None:\n",
    "            print(f\"  Skipping k={k}: metrics not available (labels may be degenerate).\")\n",
    "            continue\n",
    "        auc = metrics['roc_auc']\n",
    "        results_by_k[k] = {\n",
    "            'scores': scores,\n",
    "            'time_stats': time_stats,\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        print(f\"  k={k}: ROC AUC={auc:.4f}\")\n",
    "        if auc > best_auc or (np.isclose(auc, best_auc) and (best_k is None or k < best_k)):\n",
    "            best_auc = auc\n",
    "            best_k = k\n",
    "\n",
    "    if best_k is None:\n",
    "        raise RuntimeError(\"Could not determine best k: no valid metrics computed.\")\n",
    "\n",
    "    print(f\"\\n>>> Best k chosen by ROC AUC: k={best_k} (AUC={best_auc:.4f})\")\n",
    "    return best_k, results_by_k\n",
    "\n",
    "\n",
    "def process_dataset(dataset_filepath,\n",
    "                    dataset_fraction,\n",
    "                    k_values,\n",
    "                    contamination,\n",
    "                    n_runs,\n",
    "                    fastlof_thresholds,\n",
    "                    min_chunk_size,\n",
    "                    max_chunk_size,\n",
    "                    chunk_interval, \n",
    "                    find_best_k=True,\n",
    "                    default_k=20):\n",
    "    \"\"\"Full pipeline: load dataset, pick best k via LOF auto, run LOF/ FastLOF / RangedLOF,\n",
    "    and generate plots & timing CSV for each FastLOF threshold.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing dataset: {dataset_filepath} ===\")\n",
    "    X, y = load_dataset(dataset_filepath, dataset_fraction)\n",
    "    if X is None:\n",
    "        raise RuntimeError(\"Dataset could not be loaded.\")\n",
    "\n",
    "    print_dataset_info(X, y)\n",
    "\n",
    "    print(\"\\nNormalizing data...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    print(\"Data normalized using StandardScaler.\")\n",
    "\n",
    "    # 1) Choose best k using LOF auto\n",
    "    if find_best_k :\n",
    "        best_k, _ = find_best_k_lof_auto(X_normalized, y, k_values, contamination, 1)\n",
    "    else:\n",
    "        best_k = default_k\n",
    "    # 2) Run LOF auto with best k and get metrics\n",
    "    lof_auto_scores, lof_auto_time_stats = run_lof_auto(\n",
    "        X_normalized, best_k, contamination, n_runs\n",
    "    )\n",
    "    lof_auto_metrics = _compute_anomaly_metrics(y, lof_auto_scores, contamination) if y is not None else None\n",
    "\n",
    "    # 3) Run LOF brute with best k\n",
    "    lof_brute_scores, lof_brute_time_stats = run_lof_brute(\n",
    "        X_normalized, best_k, contamination, n_runs\n",
    "    )\n",
    "\n",
    "    # 4) Run Ranged LOF\n",
    "    ranged_lof_scores, ranged_lof_time_stats = run_ranged_lof(\n",
    "        X_normalized, best_k, contamination, n_runs, 10\n",
    "    )\n",
    "    ranged_lof_metrics = _compute_anomaly_metrics(y, ranged_lof_scores, contamination) if y is not None else None\n",
    "\n",
    "    all_results = {\n",
    "        'dataset': dataset_filepath,\n",
    "        'best_k': best_k,\n",
    "        'lof_auto_scores': lof_auto_scores,\n",
    "        'lof_auto_time_stats': lof_auto_time_stats,\n",
    "        'lof_auto_metrics': lof_auto_metrics,\n",
    "        'lof_brute_scores': lof_brute_scores,\n",
    "        'lof_brute_time_stats': lof_brute_time_stats,\n",
    "        'ranged_lof_scores': ranged_lof_scores,\n",
    "        'ranged_lof_time_stats': ranged_lof_time_stats,\n",
    "        'ranged_lof_metrics': ranged_lof_metrics,\n",
    "        'fastlof_results_by_threshold': {},\n",
    "    }\n",
    "\n",
    "    # 5) For each FastLOF threshold, run chunked FastLOF, plots, and timing CSV\n",
    "    for thr in fastlof_thresholds:\n",
    "        print(f\"\\n=== FastLOF experiments for threshold={thr} ===\")\n",
    "        fastlof_results = run_fastlof_multiple_chunks(\n",
    "            X_normalized, best_k, contamination, n_runs, X_normalized.shape[0],\n",
    "            min_chunk_size, max_chunk_size, chunk_interval, thr,\n",
    "            y_true=y,\n",
    "        )\n",
    "\n",
    "        # Add correlation info (for logging consistency)\n",
    "        print(f\"\\n--- FastLOF Correlation Results vs LOF (auto) (threshold={thr}) ---\")\n",
    "        for result in fastlof_results:\n",
    "            corr = np.corrcoef(lof_auto_scores, result['avg_scores'])[0, 1]\n",
    "            result['correlation'] = corr\n",
    "            metrics = result.get('metrics')\n",
    "            metrics_str = f\", ROC AUC = {metrics['roc_auc']:.4f}\" if metrics and metrics.get('roc_auc') is not None else \"\"\n",
    "            print(f\"Chunk count {result['chunk_count']:3d} (size {result['chunk_size']:5d}): \"\n",
    "                  f\"correlation = {corr:.6f}{metrics_str}\")\n",
    "\n",
    "        all_results['fastlof_results_by_threshold'][thr] = fastlof_results\n",
    "\n",
    "        # Create plots and timing CSV for this threshold\n",
    "        create_plots(\n",
    "            lof_auto_scores,\n",
    "            lof_auto_time_stats,\n",
    "            lof_brute_time_stats,\n",
    "            fastlof_results,\n",
    "            n_runs,\n",
    "            thr,\n",
    "            dataset_fraction,\n",
    "            dataset_filepath,\n",
    "            best_k,\n",
    "            lof_auto_metrics=lof_auto_metrics,\n",
    "            ranged_lof_time_stats=ranged_lof_time_stats,\n",
    "            ranged_lof_metrics=ranged_lof_metrics,\n",
    "            ranged_lof_scores=ranged_lof_scores,\n",
    "        )\n",
    "\n",
    "        save_timing_csv(\n",
    "            lof_auto_time_stats,\n",
    "            lof_brute_time_stats,\n",
    "            fastlof_results,\n",
    "            dataset_filepath,\n",
    "            best_k,\n",
    "            contamination,\n",
    "            n_runs,\n",
    "            thr,\n",
    "            ranged_lof_time_stats=ranged_lof_time_stats,\n",
    "        )\n",
    "\n",
    "    print(\"\\n=== Finished processing dataset ===\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def print_dataset_info(X, y):\n",
    "    \"\"\"Prints a summary of the loaded dataset.\"\"\"\n",
    "    print(\"\\n--- Dataset Exploration ---\")\n",
    "    print(f\"Filepath: {DATASET_FILEPATH}\")\n",
    "    print(f\"Fraction Used: {DATASET_FRACTION}\")\n",
    "    print(f\"Data Shape (X): {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    if y is not None:\n",
    "        num_anomalies = np.sum(y)\n",
    "        total_samples = y.shape[0]\n",
    "        anomaly_perc = (num_anomalies / total_samples) * 100 if total_samples > 0 else 0\n",
    "        print(f\"Labels Found: Yes (y shape: {y.shape})\")\n",
    "        print(f\"Anomalies (1s): {num_anomalies}\")\n",
    "        print(f\"Normals (0s): {total_samples - num_anomalies}\")\n",
    "        print(f\"Anomaly Percentage: {anomaly_perc:.4f}%\")\n",
    "        print(f\"Contamination Rate (Targeted): {CONTAMINATION_RATE}\")\n",
    "        if np.abs(anomaly_perc/100 - CONTAMINATION_RATE) > 0.05:\n",
    "             print(\" **WARNING:** Actual anomaly percentage differs significantly from target contamination rate.\")\n",
    "    else:\n",
    "        print(\"Labels Found: No (Unsupervised setup)\")\n",
    "    print(\"----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Pen Local Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/pen-local-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0015,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=50,     \n",
    "    max_chunk_size=3000,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Split Code Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define Dataset and Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILEPATH = \"data/pen-local-unsupervised-ad.csv\"\n",
    "DATASET_FRACTION = 1.0\n",
    "K_NEIGHBORS = 20\n",
    "CONTAMINATION_RATE = 0.0015\n",
    "\n",
    "N_RUNS = 10\n",
    "\n",
    "FASTLOF_THRESHOLD = 1.1\n",
    "MIN_CHUNK_SIZE = 50\n",
    "MAX_CHUNK_SIZE = 3000\n",
    "CHUNK_INTERVAL = 100\n",
    "\n",
    "X, y, X_normalized = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load, Explore, and Normalize Pen Local Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X, y = load_dataset(DATASET_FILEPATH, DATASET_FRACTION)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset file not found at '{DATASET_FILEPATH}'\")\n",
    "    X, y = None, None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the dataset: {e}\")\n",
    "    X, y = None, None\n",
    "\n",
    "if X is not None:\n",
    "    print_dataset_info(X, y)\n",
    "    \n",
    "    print(\"\\nNormalizing data...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X)\n",
    "    print(\"Data normalized using StandardScaler.\")\n",
    "else:\n",
    "    print(\"Cannot proceed. Data loading failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Run LOF Brute and Auto (Averaged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_normalized is None:\n",
    "    print(\"Cannot proceed. Data is not loaded or normalized.\")\n",
    "else:\n",
    "    # 1. Run LOF (auto) n_runs times\n",
    "    lof_auto_scores, lof_auto_time_stats = run_lof_auto(\n",
    "        X_normalized, K_NEIGHBORS, CONTAMINATION_RATE, N_RUNS\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for LOF Auto (if labels exist)\n",
    "    if y is not None:\n",
    "        lof_auto_metrics = _compute_anomaly_metrics(y, lof_auto_scores, CONTAMINATION_RATE)\n",
    "        if lof_auto_metrics:\n",
    "            print(f\"LOF (auto) Metrics: ROC AUC={lof_auto_metrics['roc_auc']:.4f}, PR AUC={lof_auto_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "    # 2. Run LOF (brute) n_runs times\n",
    "    lof_brute_scores, lof_brute_time_stats = run_lof_brute(\n",
    "        X_normalized, K_NEIGHBORS, CONTAMINATION_RATE, N_RUNS\n",
    "    )\n",
    "\n",
    "    # 3. Run Ranged LOF n_runs times\n",
    "    ranged_lof_scores, ranged_lof_time_stats = run_ranged_lof(\n",
    "        X_normalized, K_NEIGHBORS, CONTAMINATION_RATE, N_RUNS\n",
    "    )\n",
    "\n",
    "    # Calculate metrics for Ranged LOF (if labels exist)\n",
    "    if y is not None:\n",
    "        ranged_lof_metrics = _compute_anomaly_metrics(y, ranged_lof_scores, CONTAMINATION_RATE)\n",
    "        if ranged_lof_metrics:\n",
    "            print(\n",
    "                f\"Ranged LOF Metrics: ROC AUC={ranged_lof_metrics['roc_auc']:.4f}, \"\n",
    "                f\"PR AUC={ranged_lof_metrics['pr_auc']:.4f}\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n LOF Brute, Auto, and Ranged LOF tests complete. Results saved to memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Run FastLOF across Chunk Range (Averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_normalized is None:\n",
    "    print(\"Cannot proceed. Data is not loaded or normalized.\")\n",
    "elif lof_auto_scores is None:\n",
    "    print(\"Cannot proceed. LOF Auto scores are required for correlation calculation.\")\n",
    "else:\n",
    "    # Run FastLOF with multiple chunk sizes\n",
    "    try:\n",
    "        fastlof_results = run_fastlof_multiple_chunks(\n",
    "            X_normalized, K_NEIGHBORS, CONTAMINATION_RATE, N_RUNS, X_normalized.shape[0],\n",
    "            MIN_CHUNK_SIZE, MAX_CHUNK_SIZE, CHUNK_INTERVAL, FASTLOF_THRESHOLD,\n",
    "            y_true=y\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- FastLOF Correlation Results (vs LOF auto) ---\")\n",
    "        for result in fastlof_results:\n",
    "            correlation = np.corrcoef(lof_auto_scores, result['avg_scores'])[0, 1]\n",
    "            result['correlation'] = correlation\n",
    "            metrics_str = \"\"\n",
    "            metrics = result.get('metrics')\n",
    "            if metrics and metrics.get('roc_auc') is not None:\n",
    "                metrics_str = f\", ROC AUC = {metrics['roc_auc']:.4f}\"\n",
    "            print(f\"Chunk count {result['chunk_count']:3d} (size {result['chunk_size']:5d}): \"\n",
    "                  f\"correlation = {correlation:.6f}{metrics_str}\")\n",
    "\n",
    "        print(\"\\n FastLOF tests complete. Results saved to memory.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\" Parameter Error: {e}\")\n",
    "        fastlof_results = None\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred during FastLOF execution: {e}\")\n",
    "        fastlof_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Create Plots and Save Timing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([lof_auto_scores is not None, lof_auto_time_stats is not None,\n",
    "        lof_brute_time_stats is not None, fastlof_results is not None]):\n",
    "    \n",
    "    # 1. Create and save plots\n",
    "    saved_plot_paths = create_plots(\n",
    "        lof_auto_scores,\n",
    "        lof_auto_time_stats,\n",
    "        lof_brute_time_stats,\n",
    "        fastlof_results,\n",
    "        N_RUNS,\n",
    "        FASTLOF_THRESHOLD,\n",
    "        DATASET_FRACTION,\n",
    "        DATASET_FILEPATH,\n",
    "        K_NEIGHBORS,\n",
    "        lof_auto_metrics=lof_auto_metrics,\n",
    "        ranged_lof_time_stats=ranged_lof_time_stats,\n",
    "        ranged_lof_metrics=ranged_lof_metrics,\n",
    "    )\n",
    "    \n",
    "    # 2. Save timing results to CSV\n",
    "    saved_csv_path = save_timing_csv(\n",
    "        lof_auto_time_stats,\n",
    "        lof_brute_time_stats,\n",
    "        fastlof_results,\n",
    "        DATASET_FILEPATH,\n",
    "        K_NEIGHBORS,\n",
    "        CONTAMINATION_RATE,\n",
    "        N_RUNS,\n",
    "        FASTLOF_THRESHOLD,\n",
    "        ranged_lof_time_stats=ranged_lof_time_stats,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExperiment finished!\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot generate plots/CSV. One or more required result variables (LOF scores/timings, FastLOF results) are missing. Please run all preceding cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Creditcard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/creditcard.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.1,\n",
    "    n_runs=2,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=10000,\n",
    "    chunk_interval=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/breast-cancer-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0272,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=20,     \n",
    "    max_chunk_size=180,\n",
    "    chunk_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/breast-cancer-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0272,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=20,     \n",
    "    max_chunk_size=180,\n",
    "    chunk_interval=10,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Artificial Unsupervised AD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/dfki-artificial-3000-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0123,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=10,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Satellite Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/satellite-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0149,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=50,     \n",
    "    max_chunk_size=2500,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/satellite-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0149,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=50,     \n",
    "    max_chunk_size=2500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Annthyroid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/annthyroid-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.0361,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=50,     \n",
    "    max_chunk_size=3500,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Pen Global Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/pen-global-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=10,     \n",
    "    max_chunk_size=300,\n",
    "    chunk_interval=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/pen-global-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=10,     \n",
    "    max_chunk_size=300,\n",
    "    chunk_interval=10,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Kdd99 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [1.1]\n",
    "k_values = [20]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data/kdd99-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,\n",
    "    contamination=0.017,\n",
    "    n_runs=2,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=10000,\n",
    "    chunk_interval=500,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# Internet Ads Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0.0, 1.0, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [1.3, 1.4, 1.5]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [1.6, 1.8, 2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [3, 5, 7]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [10, 15, 20]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\InternetAds_norm_02_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=1500,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Pen Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0, 1, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\PenDigits_withoutdupl_norm_v01.arff\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.1,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=3000,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# Mammography Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0, 1, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\mammography.mat\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=3000,\n",
    "    chunk_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0, 1, 1.01, 1.1, 1.2]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\mammography.mat\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=100,     \n",
    "    max_chunk_size=3000,\n",
    "    chunk_interval=100,\n",
    "    find_best_k=False,\n",
    "    default_k=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# Shuttle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0, 1, 1.01, 1.1, 1.2, 1.3]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\shuttle-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=500,     \n",
    "    max_chunk_size=10000,\n",
    "    chunk_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlof_thresholds = [0, 1, 1.01, 1.1, 1.2, 1.3]\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "\n",
    "res_pen_local = process_dataset(\n",
    "    dataset_filepath=\"data\\shuttle-unsupervised-ad.csv\",\n",
    "    dataset_fraction=1.0,\n",
    "    k_values=k_values,     \n",
    "    contamination=0.11,\n",
    "    n_runs=10,\n",
    "    fastlof_thresholds=fastlof_thresholds,\n",
    "    min_chunk_size=500,     \n",
    "    max_chunk_size=10000,\n",
    "    chunk_interval=500,\n",
    "    find_best_k = False,\n",
    "    default_k = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "sklearn.datasets.fetch_kddcup99(subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastLOF Env",
   "language": "python",
   "name": "fastlof-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
