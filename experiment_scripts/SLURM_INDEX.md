# SLURM System Documentation Index

Complete documentation for the automated SLURM job submission system for FastLOF experiments.

## ğŸ“š Documentation Files

| Document | Purpose | When to Read |
|----------|---------|--------------|
| **[SLURM_QUICKSTART.md](SLURM_QUICKSTART.md)** | 5-minute setup guide | **START HERE** - First time setup |
| **[SLURM_README.md](SLURM_README.md)** | Complete documentation | Detailed reference and usage |
| **[SLURM_SUMMARY.md](SLURM_SUMMARY.md)** | System overview | Understanding the whole system |
| **[README.md](README.md)** | Main experiments readme | General experiment info |

## ğŸ› ï¸ Core System Files

| File | Purpose | Type |
|------|---------|------|
| `slurm_config.yaml` | Resource configuration | Config |
| `slurm_template.sh` | Job script template | Template |
| `slurm_submit.py` | Main submission manager | Script |
| `submit_jobs.sh` | Interactive wrapper | Script |
| `check_results.py` | Progress checker | Script |
| `monitor_jobs.sh` | Live job monitor | Script |
| `setup_cluster.sh` | Initial setup helper | Script |

## ğŸš€ Quick Command Reference

### Setup (First Time)

```bash
# Run setup script
./setup_cluster.sh

# Or manually:
chmod +x *.sh *.py
pip install -r requirements_cluster.txt
vim slurm_config.yaml  # Update work_dir
```

### Main Operations

```bash
# Test configuration
python slurm_submit.py --dry-run

# Submit all jobs
python slurm_submit.py

# Submit specific datasets
python slurm_submit.py --dataset dataset-name

# Resume after interruption
python slurm_submit.py --resume

# Check progress
python check_results.py

# Monitor jobs
./monitor_jobs.sh
```

### Job Management

```bash
# View your jobs
squeue -u $USER

# Cancel job
scancel <job_id>

# Cancel all
scancel -u $USER

# View logs
tail -f slurm_logs/dataset_fastlof_live.log
```

## ğŸ“‹ Workflows by Use Case

### Use Case 1: First-Time User

1. Read **[SLURM_QUICKSTART.md](SLURM_QUICKSTART.md)**
2. Run `./setup_cluster.sh`
3. Test: `python slurm_submit.py --dry-run`
4. Submit test job: `python slurm_submit.py --dataset annthyroid-unsupervised-ad`
5. Verify results: `python check_results.py`
6. Submit all: `python slurm_submit.py`

### Use Case 2: Troubleshooting Issues

1. Check **[SLURM_README.md](SLURM_README.md)** â†’ Troubleshooting section
2. Review logs in `slurm_logs/`
3. Check job status: `scontrol show job <job_id>`
4. Adjust `slurm_config.yaml` if needed
5. Resume: `python slurm_submit.py --resume`

### Use Case 3: Customizing Resources

1. Identify dataset in **[SLURM_README.md](SLURM_README.md)** â†’ Resource Allocation
2. Edit `slurm_config.yaml`:
   ```yaml
   datasets:
     your-dataset:
       cpus: 32        # Increase
       memory: "128G"  # Increase
       time: "48:00:00" # Extend
   ```
3. Regenerate: `python slurm_submit.py --generate-only`
4. Submit: `python slurm_submit.py --dataset your-dataset`

### Use Case 4: Monitoring Progress

```bash
# Option 1: Results checker
python check_results.py

# Option 2: Live monitor
./monitor_jobs.sh 60

# Option 3: SLURM commands
squeue -u $USER
sacct -u $USER --starttime=today

# Option 4: Log files
ls -lht slurm_logs/
tail -f slurm_logs/dataset_fastlof_live.log
```

## ğŸ¯ Dataset Reference

| Dataset | Size | Priority | Est. Time | Resources |
|---------|------|----------|-----------|-----------|
| annthyroid | Small | 1 | ~12h | 16 CPUs, 32GB |
| breast-cancer | Small | 2 | ~8h | 16 CPUs, 32GB |
| dfki-artificial-3000 | Small | 3 | ~6h | 16 CPUs, 32GB |
| pen-local | Medium | 4 | ~24h | 20 CPUs, 48GB |
| pen-global | Medium | 5 | ~24h | 20 CPUs, 48GB |
| PenDigits | Medium | 6 | ~20h | 20 CPUs, 48GB |
| InternetAds | Medium | 7 | ~30h | 24 CPUs, 64GB |
| mammography | Medium | 8 | ~36h | 24 CPUs, 64GB |
| satellite | Medium | 9 | ~30h | 24 CPUs, 64GB |
| shuttle | Medium | 10 | ~36h | 24 CPUs, 64GB |
| creditcard | Large | 11 | ~48-60h | 32 CPUs, 128GB |
| kdd99 | Large | 12 | ~60-72h | 32 CPUs, 256GB |

**Total: 12 datasets, ~3-5 days with 4 concurrent jobs**

## ğŸ”§ Configuration Reference

### slurm_config.yaml Key Settings

```yaml
global_settings:
  num_threads: 5              # IMPORTANT: Prevents thread overhead
  work_dir: "/path/to/project"  # MUST UPDATE for your cluster
  email: null                 # Optional: notifications

submission:
  max_concurrent_jobs: 4      # How many jobs at once
  check_interval: 300         # Status check frequency (seconds)
```

### Partition Options

| Partition | Nodes | MaxMem | MaxTime | Best For |
|-----------|-------|--------|---------|----------|
| `cpu` | 68 | 384GB | 3 days | Most datasets âœ“ |
| `highmem` | 4 | 2.3TB | 3 days | Very large datasets |
| `cpu_il` | 264 | 256GB | 3 days | High availability |

## ğŸ› Common Issues Quick Fix

| Issue | Quick Fix |
|-------|-----------|
| Job won't start | Check: `sinfo -p cpu` for availability |
| Module not found | Run: `module load python/3.9` or activate venv |
| Out of memory | Edit `slurm_config.yaml`: increase `memory` |
| Job too slow | Check logs for thread limit = 5 |
| Permission denied | Run: `chmod +x *.sh *.py` |
| Config error | Run: `./setup_cluster.sh` to reset |

## ğŸ“ Getting Help

1. **Quick issues**: See [SLURM_README.md](SLURM_README.md) â†’ Troubleshooting
2. **Setup problems**: Re-run `./setup_cluster.sh`
3. **Configuration**: See [SLURM_README.md](SLURM_README.md) â†’ Customization
4. **Understanding system**: Read [SLURM_SUMMARY.md](SLURM_SUMMARY.md)
5. **Logs**: Check `slurm_logs/` directory

## ğŸ“‚ Directory Structure

```
experiment_scripts/
â”œâ”€â”€ Documentation
â”‚   â”œâ”€â”€ SLURM_INDEX.md ............... (This file)
â”‚   â”œâ”€â”€ SLURM_QUICKSTART.md .......... Quick setup
â”‚   â”œâ”€â”€ SLURM_README.md .............. Complete docs
â”‚   â”œâ”€â”€ SLURM_SUMMARY.md ............. System overview
â”‚   â””â”€â”€ README.md .................... General info
â”‚
â”œâ”€â”€ Core System
â”‚   â”œâ”€â”€ slurm_config.yaml ............ Resource configuration
â”‚   â”œâ”€â”€ slurm_template.sh ............ Job template
â”‚   â”œâ”€â”€ slurm_submit.py .............. Main submission script
â”‚   â”œâ”€â”€ submit_jobs.sh ............... Interactive wrapper
â”‚   â”œâ”€â”€ check_results.py ............. Progress checker
â”‚   â”œâ”€â”€ monitor_jobs.sh .............. Live monitor
â”‚   â””â”€â”€ setup_cluster.sh ............. Setup helper
â”‚
â”œâ”€â”€ Generated (auto-created)
â”‚   â”œâ”€â”€ slurm_jobs/ .................. Individual job scripts
â”‚   â”œâ”€â”€ slurm_logs/ .................. Output and error logs
â”‚   â””â”€â”€ .slurm_state.yaml ............ State tracking
â”‚
â””â”€â”€ Experiment Scripts
    â”œâ”€â”€ annthyroid-unsupervised-ad/
    â”‚   â”œâ”€â”€ run_fastlof.py
    â”‚   â””â”€â”€ run_original_lof.py
    â”œâ”€â”€ breast-cancer-unsupervised-ad/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ... (12 datasets total)
```

## âœ… Pre-Flight Checklist

Before submitting jobs, ensure:

- [ ] Read [SLURM_QUICKSTART.md](SLURM_QUICKSTART.md)
- [ ] Ran `./setup_cluster.sh` successfully
- [ ] Updated `work_dir` in `slurm_config.yaml`
- [ ] Tested with `python slurm_submit.py --dry-run`
- [ ] Python packages installed (`pip list | grep sklearn`)
- [ ] On correct cluster partition (check `sinfo`)
- [ ] Have disk space for results (~10GB+)
- [ ] Set up tmux/screen for long-running monitor

## ğŸ“ Learning Path

**Beginner**: Just want it to work
1. Read: SLURM_QUICKSTART.md
2. Run: `./setup_cluster.sh`
3. Submit: `python slurm_submit.py`

**Intermediate**: Want to understand and customize
1. Read: SLURM_QUICKSTART.md
2. Read: SLURM_SUMMARY.md
3. Customize: `slurm_config.yaml`
4. Submit phased: Small â†’ Medium â†’ Large datasets

**Advanced**: Need full control
1. Read: All documentation
2. Generate jobs: `python slurm_submit.py --generate-only`
3. Customize: Edit individual job scripts in `slurm_jobs/`
4. Submit: Manual `sbatch` or via submission script
5. Monitor: Custom scripts + SLURM commands

## ğŸ¯ Success Metrics

You'll know the system is working when:

1. âœ“ `--dry-run` shows expected jobs
2. âœ“ Jobs appear in `squeue -u $USER`
3. âœ“ Log files being created in `slurm_logs/`
4. âœ“ `check_results.py` shows progress
5. âœ“ Results appearing in `../results/<dataset>/fastlof_experiments/`

## ğŸ“Š Expected Outcomes

After all jobs complete (~5 days):

```
results/
â”œâ”€â”€ annthyroid-unsupervised-ad/
â”‚   â””â”€â”€ fastlof_experiments/
â”‚       â””â”€â”€ threshold_1.1/
â”‚           â”œâ”€â”€ results_summary.csv ........ Summary of all k-values
â”‚           â”œâ”€â”€ comparison_plot.png ........ Visualization
â”‚           â””â”€â”€ detailed_results.csv ....... Full metrics per run
â”œâ”€â”€ breast-cancer-unsupervised-ad/
â”‚   â””â”€â”€ ... (same structure)
...
â””â”€â”€ kdd99-unsupervised-ad/
    â””â”€â”€ ... (same structure)

Total: 12 datasets Ã— multiple k-values = comprehensive results
```

## ğŸš€ Ready to Start?

```bash
# Your journey begins here:
cat SLURM_QUICKSTART.md

# Or jump right in:
./setup_cluster.sh
```

Good luck with your experiments! ğŸ‰
